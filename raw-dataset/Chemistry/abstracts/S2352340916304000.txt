Artifact rejection techniques are used to recover the brain signals underlying artifactual electroencephalographic (EEG) segments. Although over the last few years many different artifact rejection techniques have been proposed (http://dx.doi.org/10.1109/JSEN.2011.2115236 [1], http://dx.doi.org/10.1016/j.clinph.2006.09.003 [2], http://dx.doi.org/10.3390/e16126553 [3]), none has been established as a gold standard so far, because assessing their performance is difficult and subjective (http://dx.doi.org/10.1109/ITAB.2009.5394295 [4], http://dx.doi.org/10.1016/j.bspc.2011.02.001 [5], http://dx.doi.org/10.1007/978-3-540-89208-3_300. [6]). This limitation is mainly based on the fact that the underlying artifact-free brain signal is unknown, so there is no objective way to measure how close the retrieved signal is to the real one. This article solves the aforementioned problem by presenting a semi-simulated EEG dataset, where artifact-free EEG signals are manually contaminated with ocular artifacts, using a realistic head model. The significant part of this dataset is that it contains the pre-contamination EEG signals, so the brain signals underlying the EOG artifacts are known and thus the performance of every artifact rejection technique can be objectively assessed.