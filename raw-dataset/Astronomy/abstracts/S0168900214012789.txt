Avalanche fluctuations set a limit to the energy and position resolutions that can be reached by gaseous detectors. This paper presents a method based on a laser test-bench to measure the absolute gain and the relative gain variance of a Micro-Pattern Gaseous Detector from its single-electron response. A Micromegas detector was operated with three binary gas mixtures, composed of 5% isobutane as a quencher, with argon, neon or helium, at atmospheric pressure. The anode signals were read out by low-noise, high-gain Cremat CR-110 charge preamplifiers to enable single-electron detection down to gain of 5× 103 for the first time. The argon mixture shows the lowest gain at a given amplification field together with the lowest breakdown limit, which is at a gain of 2×104 an order of magnitude lower than that of neon or helium. For each gas, the relative gain variance f is almost unchanged in the range of amplification field studied. It was found that f is twice higher (f~0.6) in argon than in the two other mixtures. This hierarchy of gain and relative gain variance agrees with predictions of analytic models, based on gas ionisation yields, and a Monte-Carlo model included in the simulation software Magboltz version 10.1.