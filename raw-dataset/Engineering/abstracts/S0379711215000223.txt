Polymer pyrolysis is a key phenomenon in solid ignition, flame spread and fire growth. It is therefore an essential part in the understanding of fire behaviour. Advances in computational pyrolysis during the last decade have mainly resulted in an increase of the number of physical and chemical mechanisms implemented in the models. This stems from the implicit assumption that models with a higher level of complexity should be more accurate. However, a direct consequence of this growth in complexity is the addition of new parameters and the accumulation of modelling uncertainty born from the lack of knowledge of their values. The large number of parameters and the difficulty to quantify their values from direct measurements often oblige modellers to solve an inverse problem to perform the calibration of their models. By doing inverse modelling, the equations and the experimental data are consequently coupled to the parameter values found. This coupling and its consequences, which are most often ignored, are investigated here using different levels of model complexity for the simulation in Gpyro of transient pyrolysis of PolyMethylMethAcrylate in non-burning conditions. Among the wide range of possible model complexities, five models with a number of parameters ranging from 3 to 30 are considered. It is observed that models of different complexities (i.e. number of mechanisms and associated assumptions) can achieve similar levels of accuracy by virtue of using different parameters values. The results show the strong presence of multiple compensation effects between implemented mechanisms (e.g. chemistry or heat transfer), and that an increase of model complexity can induce a large scatter in the parameters values found. We recommend the use of larger data sets from different experimental procedures (e.g. different boundary conditions) and of different nature (e.g. in-depth temperature profile instead of only surface temperature) to break down the compensation effects found in this study.